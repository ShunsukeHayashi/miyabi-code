#!/usr/bin/env python3
"""
FANZAæ–°ç€ä½œå“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆAPIã‚­ãƒ¼ä¸è¦ç‰ˆï¼‰
Beautiful Soup 4ã‚’ä½¿ç”¨ã—ã¦FANZAã®æ–°ç€ä½œå“æƒ…å ±ã‚’å–å¾—

Usage:
    python3 fanza_scraper.py [--pages 3] [--output data/fanza/new_releases.md]
"""

import argparse
import json
import re
import sys
from datetime import datetime
from pathlib import Path
from time import sleep
from typing import List, Dict

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("âŒ å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("")
    print("ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚³ãƒãƒ³ãƒ‰:")
    print("   pip3 install requests beautifulsoup4")
    print("")
    print("ã¾ãŸã¯:")
    print("   python3 -m pip install requests beautifulsoup4")
    sys.exit(1)


class FANZAScraper:
    """FANZAæ–°ç€ä½œå“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""

    BASE_URL = "https://www.dmm.co.jp/digital/videoa/-/list/"
    PARAMS = "=/article=keyword/id=6529/sort=date"

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        })
        # å¹´é½¢ç¢ºèªã‚¯ãƒƒã‚­ãƒ¼ã‚’è¨­å®š
        self.session.cookies.set("age_check_done", "1", domain=".dmm.co.jp")
        self.session.cookies.set("ckcy", "1", domain=".dmm.co.jp")

    def fetch_page(self, page: int = 1) -> BeautifulSoup:
        """æŒ‡å®šãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—"""
        url = f"{self.BASE_URL}{self.PARAMS}/page={page}/"
        print(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page} ã‚’å–å¾—ä¸­: {url}")

        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return BeautifulSoup(response.content, "html.parser")
        except requests.RequestException as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def extract_items(self, soup: BeautifulSoup) -> List[Dict]:
        """ãƒšãƒ¼ã‚¸ã‹ã‚‰ä½œå“æƒ…å ±ã‚’æŠ½å‡º"""
        items = []

        # å•†å“ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆå®Ÿéš›ã®HTMLæ§‹é€ ã«å¿œã˜ã¦èª¿æ•´ãŒå¿…è¦ï¼‰
        # DMM/FANZAã®HTMLæ§‹é€ ã®ä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³:
        # - id="list" å†…ã® ul > li
        # - class="tmb" ãŒã‚µãƒ ãƒã‚¤ãƒ«/ã‚¿ã‚¤ãƒˆãƒ«
        # - class="tx-text" ãŒãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±

        product_list = soup.find(id="list")
        if not product_list:
            print("âš ï¸  å•†å“ãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆå¹´é½¢ç¢ºèªãƒšãƒ¼ã‚¸ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰")
            return items

        product_items = product_list.find_all("li")
        print(f"   â†’ {len(product_items)}ä»¶ã®å•†å“ã‚’ç™ºè¦‹")

        for item in product_items:
            try:
                # ã‚¿ã‚¤ãƒˆãƒ«
                title_elem = item.find("p", class_="tmb")
                title = title_elem.get_text(strip=True) if title_elem else "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"

                # ãƒªãƒ³ã‚¯
                link_elem = item.find("a")
                link = link_elem["href"] if link_elem and link_elem.get("href") else ""
                if link and not link.startswith("http"):
                    link = "https://www.dmm.co.jp" + link

                # ã‚µãƒ ãƒã‚¤ãƒ«
                img_elem = item.find("img")
                thumbnail = img_elem["src"] if img_elem and img_elem.get("src") else ""

                # å“ç•ªï¼ˆURLã‹ã‚‰æŠ½å‡ºã‚’è©¦ã¿ã‚‹ï¼‰
                product_id = ""
                if link:
                    match = re.search(r'cid=([^/&]+)', link)
                    if match:
                        product_id = match.group(1)

                # ä¾¡æ ¼ï¼ˆã‚ã‚Œã°ï¼‰
                price_elem = item.find("p", class_="price")
                price = price_elem.get_text(strip=True) if price_elem else "ä¾¡æ ¼ä¸æ˜"

                items.append({
                    "title": title,
                    "product_id": product_id,
                    "url": link,
                    "thumbnail": thumbnail,
                    "price": price,
                    "scraped_at": datetime.now().isoformat(),
                })

            except Exception as e:
                print(f"âš ï¸  å•†å“æƒ…å ±ã®æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                continue

        return items

    def scrape(self, pages: int = 1) -> List[Dict]:
        """è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        all_items = []

        for page in range(1, pages + 1):
            soup = self.fetch_page(page)
            if not soup:
                break

            items = self.extract_items(soup)
            all_items.extend(items)

            print(f"   âœ… {len(items)}ä»¶å–å¾— (åˆè¨ˆ: {len(all_items)}ä»¶)")

            # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ã®å¾…æ©Ÿ
            if page < pages:
                sleep(2)

        return all_items


def save_markdown(items: List[Dict], output_path: Path):
    """Markdownå½¢å¼ã§ä¿å­˜"""
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(f"# FANZA æ–°ç€ä½œå“ - {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}\n\n")
        f.write(f"**å–å¾—æ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**å–å¾—ä»¶æ•°**: {len(items)}ä»¶\n\n")
        f.write("---\n\n")

        for item in items:
            f.write(f"## {item['title']}\n\n")
            if item['product_id']:
                f.write(f"- **å“ç•ª**: {item['product_id']}\n")
            f.write(f"- **ä¾¡æ ¼**: {item['price']}\n")
            f.write(f"- **URL**: {item['url']}\n\n")
            if item['thumbnail']:
                f.write(f"![ã‚µãƒ ãƒã‚¤ãƒ«]({item['thumbnail']})\n\n")
            f.write("---\n\n")

    print(f"âœ… Markdownä¿å­˜: {output_path}")


def save_json(items: List[Dict], output_path: Path):
    """JSONå½¢å¼ã§ä¿å­˜"""
    data = {
        "date": datetime.now().isoformat(),
        "count": len(items),
        "items": items,
    }

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"âœ… JSONä¿å­˜: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="FANZAæ–°ç€ä½œå“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼")
    parser.add_argument("--pages", type=int, default=1, help="å–å¾—ãƒšãƒ¼ã‚¸æ•° (default: 1)")
    parser.add_argument("--output", type=str, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (default: data/fanza/new_releases_YYYYMMDD.md)")
    args = parser.parse_args()

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = Path("data/fanza")
    output_dir.mkdir(parents=True, exist_ok=True)

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åæ±ºå®š
    date_str = datetime.now().strftime("%Y%m%d")
    if args.output:
        output_md = Path(args.output)
    else:
        output_md = output_dir / f"new_releases_{date_str}.md"

    output_json = output_md.with_suffix(".json")

    print("ğŸ” FANZAæ–°ç€ä½œå“ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
    print(f"   å–å¾—ãƒšãƒ¼ã‚¸æ•°: {args.pages}")
    print(f"   å‡ºåŠ›å…ˆ: {output_md}")
    print("")

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    scraper = FANZAScraper()
    items = scraper.scrape(pages=args.pages)

    if not items:
        print("âŒ ä½œå“æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        sys.exit(1)

    # ä¿å­˜
    save_markdown(items, output_md)
    save_json(items, output_json)

    print("")
    print(f"ğŸ“Š å–å¾—ä½œå“æ•°: {len(items)}")
    print("ğŸ‰ å®Œäº†ï¼")


if __name__ == "__main__":
    main()
