name: SWE-bench Pro Benchmark

on:
  workflow_dispatch:
    inputs:
      instance_limit:
        description: 'Number of instances to evaluate (default: 10 for testing)'
        required: false
        default: '10'
      run_id:
        description: 'Custom run ID (default: auto-generated)'
        required: false
        default: ''
      max_workers:
        description: 'Number of parallel Docker containers (default: 2)'
        required: false
        default: '2'
  schedule:
    # Run weekly on Sunday at 00:00 UTC
    - cron: '0 0 * * 0'

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  benchmark:
    name: Run SWE-bench Pro Evaluation
    runs-on: ubuntu-latest
    timeout-minutes: 480  # 8 hours max

    # Resource requirements
    # - Storage: 120GB (SWE-bench Docker images)
    # - RAM: 16GB minimum
    # - CPU: 8 cores recommended

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Free up disk space
        run: |
          echo "=== Before cleanup ==="
          df -h

          # Remove unnecessary packages
          sudo apt-get remove -y '^aspnetcore-.*' '^dotnet-.*' '^llvm-.*' '^php.*' azure-cli \
            google-chrome-stable firefox powershell mono-devel
          sudo apt-get autoremove -y
          sudo apt-get clean

          # Remove Docker images
          docker image prune -af

          # Remove swap
          sudo swapoff -a
          sudo rm -f /swapfile

          echo "=== After cleanup ==="
          df -h

      - name: Setup Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      - name: Setup Rust cache
        uses: Swatinem/rust-cache@v2
        with:
          prefix-key: "benchmark"
          shared-key: "swe-bench-pro"

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install SWE-bench official harness
        run: |
          echo "üì• Installing SWE-bench official harness..."
          git clone --depth 1 https://github.com/princeton-nlp/SWE-bench.git /tmp/SWE-bench
          cd /tmp/SWE-bench
          pip install -e .

          echo "‚úÖ SWE-bench harness installed"
          python -m swebench.harness.run_evaluation --help

      - name: Setup Docker
        run: |
          echo "üê≥ Setting up Docker..."

          # Increase Docker storage
          sudo systemctl stop docker
          sudo rm -rf /var/lib/docker
          sudo mkdir -p /mnt/docker-data
          sudo ln -s /mnt/docker-data /var/lib/docker
          sudo systemctl start docker

          echo "‚úÖ Docker setup complete"
          docker info

      - name: Download SWE-bench Pro dataset
        run: |
          echo "üì• Downloading SWE-bench Pro dataset..."
          mkdir -p benchmarks/swe-bench-pro/data
          cd benchmarks/swe-bench-pro
          python scripts/download_dataset.py

          echo "‚úÖ Dataset downloaded"
          ls -lh data/

      - name: Build Miyabi benchmark CLI
        run: |
          echo "üî® Building Miyabi benchmark CLI..."
          cargo build --release --bin miyabi-benchmark

          echo "‚úÖ Build complete"
          ./target/release/miyabi-benchmark --version || echo "CLI built successfully"

      - name: Generate predictions (Miyabi evaluation)
        run: |
          echo "ü§ñ Generating predictions with Miyabi..."

          INSTANCE_LIMIT="${{ github.event.inputs.instance_limit || '10' }}"
          OUTPUT_DIR="benchmarks/swe-bench-pro/output"

          mkdir -p "$OUTPUT_DIR"

          # Note: This is a placeholder - actual implementation depends on CLI args
          cargo run --release --bin miyabi-benchmark -- \
            --dataset benchmarks/swe-bench-pro/data/swebench_pro.json \
            --output "$OUTPUT_DIR/predictions.jsonl" \
            --limit "$INSTANCE_LIMIT" \
            || echo "‚ö†Ô∏è  Prediction generation failed or not yet implemented"

          if [ -f "$OUTPUT_DIR/predictions.jsonl" ]; then
            echo "‚úÖ Predictions generated: $(wc -l < $OUTPUT_DIR/predictions.jsonl) instances"
            head -3 "$OUTPUT_DIR/predictions.jsonl"
          else
            echo "‚ùå Predictions file not found - using placeholder"
            # Create placeholder for testing
            echo '{"instance_id":"test-1","model_name_or_path":"miyabi-v1.0.0","model_patch":"placeholder"}' > "$OUTPUT_DIR/predictions.jsonl"
          fi

      - name: Run official SWE-bench harness
        id: harness
        run: |
          echo "üöÄ Running official SWE-bench evaluation harness..."

          RUN_ID="${{ github.event.inputs.run_id }}"
          if [ -z "$RUN_ID" ]; then
            RUN_ID="miyabi-$(date +%Y%m%d-%H%M%S)"
          fi

          MAX_WORKERS="${{ github.event.inputs.max_workers || '2' }}"
          PREDICTIONS_PATH="benchmarks/swe-bench-pro/output/predictions.jsonl"

          echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT

          # Run official harness
          python -m swebench.harness.run_evaluation \
            --predictions_path "$PREDICTIONS_PATH" \
            --max_workers "$MAX_WORKERS" \
            --run_id "$RUN_ID" \
            2>&1 | tee evaluation.log || true

          echo "‚úÖ Evaluation complete (check logs for details)"

      - name: Collect results
        if: always()
        run: |
          echo "üìä Collecting evaluation results..."

          RUN_ID="${{ steps.harness.outputs.run_id }}"
          RESULTS_DIR="evaluation_results/$RUN_ID"

          if [ -d "$RESULTS_DIR" ]; then
            echo "=== Results Summary ==="
            if [ -f "$RESULTS_DIR/results.json" ]; then
              cat "$RESULTS_DIR/results.json" | jq '.' || cat "$RESULTS_DIR/results.json"
            else
              echo "‚ö†Ô∏è  results.json not found"
            fi

            echo ""
            echo "=== Files generated ==="
            find "$RESULTS_DIR" -type f | head -20
          else
            echo "‚ùå Results directory not found: $RESULTS_DIR"
          fi

      - name: Generate summary report
        if: always()
        run: |
          echo "# üìä SWE-bench Pro Evaluation Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Run ID**: ${{ steps.harness.outputs.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "**Instances**: ${{ github.event.inputs.instance_limit || '10' }}" >> $GITHUB_STEP_SUMMARY
          echo "**Workers**: ${{ github.event.inputs.max_workers || '2' }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          RUN_ID="${{ steps.harness.outputs.run_id }}"
          RESULTS_FILE="evaluation_results/$RUN_ID/results.json"

          if [ -f "$RESULTS_FILE" ]; then
            echo "## ‚úÖ Results" >> $GITHUB_STEP_SUMMARY
            echo '```json' >> $GITHUB_STEP_SUMMARY
            cat "$RESULTS_FILE" | jq '.' >> $GITHUB_STEP_SUMMARY || cat "$RESULTS_FILE" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "## ‚ö†Ô∏è  Results Not Available" >> $GITHUB_STEP_SUMMARY
            echo "Check logs for details" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload predictions artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: predictions-${{ steps.harness.outputs.run_id }}
          path: benchmarks/swe-bench-pro/output/predictions.jsonl
          retention-days: 90

      - name: Upload evaluation results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results-${{ steps.harness.outputs.run_id }}
          path: |
            evaluation_results/
            evaluation.log
          retention-days: 90

      - name: Upload evaluation logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-logs-${{ steps.harness.outputs.run_id }}
          path: logs/
          retention-days: 30
          if-no-files-found: ignore

      - name: Notify on failure
        if: failure()
        run: |
          echo "‚ùå Benchmark evaluation failed"
          echo "Check the logs and artifacts for details"

  report:
    name: Generate Markdown Report
    needs: benchmark
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download evaluation results
        uses: actions/download-artifact@v4
        with:
          pattern: evaluation-results-*
          path: results/

      - name: Generate Markdown report
        run: |
          echo "# SWE-bench Pro Evaluation Report" > BENCHMARK_REPORT.md
          echo "" >> BENCHMARK_REPORT.md
          echo "**Date**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> BENCHMARK_REPORT.md
          echo "**Workflow**: ${{ github.workflow }}" >> BENCHMARK_REPORT.md
          echo "**Run**: ${{ github.run_number }}" >> BENCHMARK_REPORT.md
          echo "" >> BENCHMARK_REPORT.md

          # Find results.json files
          find results/ -name "results.json" -exec echo "## Results: {}" \; -exec cat {} \; >> BENCHMARK_REPORT.md

          echo "" >> BENCHMARK_REPORT.md
          echo "---" >> BENCHMARK_REPORT.md
          echo "*Generated by GitHub Actions*" >> BENCHMARK_REPORT.md

      - name: Upload report
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report
          path: BENCHMARK_REPORT.md
          retention-days: 90
