"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.speechToTextV1Tools = exports.speechToTextV1SpeechStreamRecognize = exports.speechToTextV1SpeechFileRecognize = void 0;
const zod_1 = require("zod");
exports.speechToTextV1SpeechFileRecognize = {
    project: 'speech_to_text',
    name: 'speech_to_text.v1.speech.fileRecognize',
    sdkName: 'speech_to_text.v1.speech.fileRecognize',
    path: '/open-apis/speech_to_text/v1/speech/file_recognize',
    httpMethod: 'POST',
    description: '[Feishu/Lark]-AI-Speech recognition-Audio file speech recognition-An audio speech recognition API is provided to recognize the entire audio file (less than 60s) at one time',
    accessTokens: ['tenant'],
    schema: {
        data: zod_1.z.object({
            speech: zod_1.z
                .object({ speech: zod_1.z.string().describe('Content of the base64-encoded audio file').optional() })
                .describe('Audio resources'),
            config: zod_1.z
                .object({
                file_id: zod_1.z
                    .string()
                    .describe('A 16-bit string generated by a user to identify a file. The string can only contain letters, numbers, and underscores'),
                format: zod_1.z.string().describe('Audio format. Only pcm supported'),
                engine_type: zod_1.z
                    .string()
                    .describe('Engine type. Only 16k_auto that allows a mix of Chinese and English is supported'),
            })
                .describe('Configures properties'),
        }),
    },
};
exports.speechToTextV1SpeechStreamRecognize = {
    project: 'speech_to_text',
    name: 'speech_to_text.v1.speech.streamRecognize',
    sdkName: 'speech_to_text.v1.speech.streamRecognize',
    path: '/open-apis/speech_to_text/v1/speech/stream_recognize',
    httpMethod: 'POST',
    description: '[Feishu/Lark]-AI-Speech recognition-Streaming speech recognition-A streaming speech recognition API is provided to input an audio clip by clip and receive recognition results in real time. Each audio clip is recommended to be within 100 to 200 ms',
    accessTokens: ['tenant'],
    schema: {
        data: zod_1.z.object({
            speech: zod_1.z
                .object({ speech: zod_1.z.string().describe('Content of the base64-encoded audio file').optional() })
                .describe('Audio resources'),
            config: zod_1.z
                .object({
                stream_id: zod_1.z
                    .string()
                    .describe('A 16-bit string generated by a user to identify the same data stream. The string can only contain letters, numbers, and underscores'),
                sequence_id: zod_1.z
                    .number()
                    .describe('Sequence number of a data stream clip, starting from 0 and incremented by 1 for each request'),
                action: zod_1.z
                    .number()
                    .describe('Indicates the data packet sent to the API in a request. 1: The first packet. 2: The last packet. A result will be returned. 3: The cancelled data packets,no final result is returned. 0: Transfer data packets in the middle of voice transmission'),
                format: zod_1.z.string().describe('Audio format. Only pcm supported'),
                engine_type: zod_1.z
                    .string()
                    .describe('Engine type. Only 16k_auto that allows a mix of Chinese and English is supported'),
            })
                .describe('Configures properties'),
        }),
    },
};
exports.speechToTextV1Tools = [exports.speechToTextV1SpeechFileRecognize, exports.speechToTextV1SpeechStreamRecognize];
