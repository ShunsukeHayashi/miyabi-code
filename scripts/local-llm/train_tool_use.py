#!/usr/bin/env python3
"""
Miyabi Local LLM Tool Use Training Script
LoRA/QLoRA ã‚’ä½¿ç”¨ã—ãŸTool Callingèƒ½åŠ›ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
"""

import json
import torch
from pathlib import Path
from datetime import datetime

# è¨­å®š
CONFIG = {
    "base_model": "Qwen/Qwen2.5-Coder-7B-Instruct",
    "output_dir": "/home/ubuntu/miyabi-private/models/miyabi-tool-use",
    "dataset_path": "/home/ubuntu/miyabi-private/data/tool_use_dataset.jsonl",
    
    # LoRAè¨­å®š
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0,
    "target_modules": [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    "learning_rate": 2e-4,
    "batch_size": 2,
    "gradient_accumulation_steps": 8,
    "num_epochs": 1,
    "max_seq_length": 2048,
    "warmup_ratio": 0.1,
}

def check_dependencies():
    """ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯"""
    required = ["transformers", "peft", "trl", "bitsandbytes", "datasets"]
    missing = []
    for pkg in required:
        try:
            __import__(pkg)
        except ImportError:
            missing.append(pkg)
    
    if missing:
        print(f"âŒ ä¸è¶³ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸: {', '.join(missing)}")
        print(f"ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install {' '.join(missing)}")
        return False
    return True

def load_dataset(path: str):
    """Tool Useãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰"""
    from datasets import load_dataset
    
    if Path(path).exists():
        return load_dataset("json", data_files=path)["train"]
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        print("ğŸ“¥ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (glaive-function-calling) ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        ds = load_dataset("glaiveai/glaive-function-calling-v2", split="train")
        return ds.select(range(min(5000, len(ds))))  # æœ€åˆã®5000ã‚µãƒ³ãƒ—ãƒ«

def create_sample_dataset():
    """Miyabi MCPãƒ„ãƒ¼ãƒ«ç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"""
    samples = [
        {
            "messages": [
                {"role": "user", "content": "tmuxã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’è¡¨ç¤ºã—ã¦"},
                {"role": "assistant", "tool_calls": [{
                    "type": "function",
                    "function": {
                        "name": "tmux_list_sessions",
                        "arguments": {}
                    }
                }]}
            ],
            "tools": [{
                "type": "function",
                "function": {
                    "name": "tmux_list_sessions",
                    "description": "List all tmux sessions",
                    "parameters": {"type": "object", "properties": {}}
                }
            }]
        },
        {
            "messages": [
                {"role": "user", "content": "Issue #123ã®è©³ç´°ã‚’è¦‹ã›ã¦"},
                {"role": "assistant", "tool_calls": [{
                    "type": "function",
                    "function": {
                        "name": "get_issue",
                        "arguments": {"issue_number": 123}
                    }
                }]}
            ],
            "tools": [{
                "type": "function",
                "function": {
                    "name": "get_issue",
                    "description": "Get GitHub issue details",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "issue_number": {"type": "integer"}
                        },
                        "required": ["issue_number"]
                    }
                }
            }]
        },
        # è¿½åŠ ã‚µãƒ³ãƒ—ãƒ«...
    ]
    
    output_path = Path(CONFIG["dataset_path"])
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w") as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")
    
    print(f"âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ: {output_path}")
    return samples

def train():
    """ãƒ¡ã‚¤ãƒ³ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•°"""
    print("ğŸš€ Miyabi Tool Use Training")
    print("=" * 50)
    
    if not check_dependencies():
        return
    
    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
    from peft import LoraConfig, get_peft_model
    from trl import SFTTrainer, SFTConfig
    
    # é‡å­åŒ–è¨­å®š
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    
    print(f"ğŸ“¥ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­: {CONFIG['base_model']}")
    model = AutoModelForCausalLM.from_pretrained(
        CONFIG["base_model"],
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
    )
    
    tokenizer = AutoTokenizer.from_pretrained(
        CONFIG["base_model"],
        trust_remote_code=True,
    )
    tokenizer.pad_token = tokenizer.eos_token
    
    # LoRAè¨­å®š
    lora_config = LoraConfig(
        r=CONFIG["lora_r"],
        lora_alpha=CONFIG["lora_alpha"],
        lora_dropout=CONFIG["lora_dropout"],
        target_modules=CONFIG["target_modules"],
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    dataset = load_dataset(CONFIG["dataset_path"])
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(dataset)}")
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    training_args = SFTConfig(
        output_dir=CONFIG["output_dir"],
        per_device_train_batch_size=CONFIG["batch_size"],
        gradient_accumulation_steps=CONFIG["gradient_accumulation_steps"],
        learning_rate=CONFIG["learning_rate"],
        lr_scheduler_type="linear",
        warmup_ratio=CONFIG["warmup_ratio"],
        num_train_epochs=CONFIG["num_epochs"],
        max_seq_length=CONFIG["max_seq_length"],
        optim="adamw_8bit",
        bf16=True,
        gradient_checkpointing=True,
        logging_steps=10,
        save_steps=100,
    )
    
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        processing_class=tokenizer,
    )
    
    print("ğŸ‹ï¸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
    trainer.train()
    
    # ä¿å­˜
    output_path = Path(CONFIG["output_dir"]) / f"checkpoint-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
    trainer.save_model(output_path)
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {output_path}")

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--create-sample":
        create_sample_dataset()
    else:
        train()
