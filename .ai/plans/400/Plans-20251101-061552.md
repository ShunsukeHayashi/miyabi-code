# Plans for Issue #400

**Title**: ğŸ“„ [Sub-Issue #396] Phase 3: Miyabiè©•ä¾¡ãƒ©ãƒƒãƒ‘ãƒ¼ã¨ãƒ‘ãƒƒãƒç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…

**URL**: https://github.com/customer-cloud/miyabi-private/issues/400

---

## ğŸ“‹ Summary

- **Total Tasks**: 4
- **Estimated Duration**: 60 minutes
- **Execution Levels**: 4
- **Has Cycles**: âœ… No

## ğŸ“ Task Breakdown

### 1. Analyze requirements for #400

- **ID**: `task-400-analysis`
- **Type**: Docs
- **Assigned Agent**: IssueAgent
- **Priority**: 0
- **Estimated Duration**: 5 min

**Description**: Analyze issue requirements and create detailed specification

### 2. Implement solution for #400

- **ID**: `task-400-impl`
- **Type**: Feature
- **Assigned Agent**: CodeGenAgent
- **Priority**: 1
- **Estimated Duration**: 30 min
- **Dependencies**: task-400-analysis

**Description**: ## ğŸ¯ ç›®çš„

**Miyabiã§å„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å‡¦ç†ã—ã€å…¬å¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæº–æ‹ ã®ãƒ‘ãƒƒãƒã‚’ç”Ÿæˆã™ã‚‹è©•ä¾¡ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’å®Ÿè£…**

**è¦ªIssue**: #396 - SWE-bench Proè©•ä¾¡å®Ÿè£…
**ä¾å­˜**: #399 (Phase 2) - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆå®Œäº†å¾Œã«å®Ÿæ–½

---

## ğŸ“‹ ã‚¿ã‚¹ã‚¯

### 1. `miyabi-benchmark` crateä½œæˆ

```bash
# æ–°è¦crateã®ä½œæˆ
cargo new --lib crates/miyabi-benchmark

# Cargo.tomlã«è¿½åŠ 
[workspace]
members = [
    "crates/miyabi-benchmark",
    # ... æ—¢å­˜ã®crates
]
```

**æ¤œè¨¼é …ç›®**:
- [ ] crateãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã‚‹
- [ ] workspaceã«è¿½åŠ ã•ã‚Œã‚‹
- [ ] `cargo build`ãŒæˆåŠŸ

---

### 2. `SWEBenchProEvaluator` structå®Ÿè£…

```rust
// crates/miyabi-benchmark/src/evaluator.rs

use miyabi_agents::CoordinatorAgent;
use miyabi_worktree::WorktreeManager;
use miyabi_types::benchmark::{SWEBenchInstance, PatchOutput};
use anyhow::Result;
use std::time::Duration;

pub struct SWEBenchProEvaluator {
    coordinator: CoordinatorAgent,
    worktree_manager: WorktreeManager,
    config: BenchmarkConfig,
}

#[derive(Debug, Clone)]
pub struct BenchmarkConfig {
    /// ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰
    pub timeout: Duration,
    
    /// ä¸¦åˆ—å®Ÿè¡Œæ•°
    pub concurrency: usize,
    
    /// Worktreeãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    pub worktree_base: String,
}

impl SWEBenchProEvaluator {
    pub fn new(config: BenchmarkConfig) -> Result<Self> {
        Ok(Self {
            coordinator: CoordinatorAgent::new()?,
            worktree_manager: WorktreeManager::new(&config.worktree_base)?,
            config,
        })
    }
    
    /// å˜ä¸€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®è©•ä¾¡
    pub async fn evaluate_instance(&self, instance: &SWEBenchInstance) -> Result<PatchOutput> {
        // 1. Worktreeä½œæˆï¼ˆbase_commitã§checkoutï¼‰
        let wt = self.worktree_manager.create(
            &instance.repo,
            &instance.base_commit,
        )?;
        
        // 2. å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
        let context = self.create_execution_context(instance)?;
        wt.write_context(&context)?;
        
        // 3. CoordinatorAgentå®Ÿè¡Œï¼ˆClaude CodeçµŒç”±ï¼‰
        let result = tokio::time::timeout(
            self.config.timeout,
            self.coordinator.execute_in_worktree(&wt, &instance.problem_statement)
        ).await??;
        
        // 4. git diffã§ãƒ‘ãƒƒãƒç”Ÿæˆï¼ˆå…¬å¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰
        let patch = self.generate_official_patch(&wt, &instance.base_commit)?;
        
        // 5. Worktreeã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self.worktree_manager.remove(&wt)?;
        
        // 6. å…¬å¼JSONå½¢å¼ã§å‡ºåŠ›
        Ok(PatchOutput {
            instance_id: instance.instance_id.clone(),
            model_patch: patch,
            model_name_or_path: "miyabi-v1.0.0".to_string(),
        })
    }
    
    /// è¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä¸¦åˆ—è©•ä¾¡
    pub async fn evaluate_batch(&self, instances: &[SWEBenchInstance]) -> Result<Vec<PatchOutput>> {
        use futures::stream::{self, StreamExt};
        
        let results = stream::iter(instances)
            .map(|inst| self.evaluate_instance(inst))
            .buffer_unordered(self.config.concurrency)
            .collect::<Vec<_>>()
            .await;
        
        results.into_iter().collect()
    }
}
```

**æ¤œè¨¼é …ç›®**:
- [ ] Worktreeä½œæˆãŒæ­£å¸¸ã«å‹•ä½œ
- [ ] ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒæ­£ã—ãæ©Ÿèƒ½
- [ ] ä¸¦åˆ—å®Ÿè¡ŒãŒå®‰å®š
- [ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒé©åˆ‡

---

### 3. ãƒ‘ãƒƒãƒç”Ÿæˆé–¢æ•°å®Ÿè£…

```rust
// crates/miyabi-benchmark/src/patch.rs

use std::process::Command;
use anyhow::{Result, Context};

/// å…¬å¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæº–æ‹ ã®ãƒ‘ãƒƒãƒç”Ÿæˆ
pub fn generate_official_patch(
    worktree_path: &str,
    base_commit: &str,
) -> Result<String> {
    // unified diffå½¢å¼ï¼ˆ--unified=3ï¼‰
    let output = Command::new("git")
        .args(&[
            "diff",
            "--unified=3",  // å…¬å¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            base_commit,
            "HEAD",
        ])
        .current_dir(worktree_path)
        .output()
        .context("Failed to execute git diff")?;
    
    if !output.status.success() {
        anyhow::bail!(
            "git diff failed: {}",
            String::from_utf8_lossy(&output.stderr)
        );
    }
    
    let patch = String::from_utf8(output.stdout)
        .context("Invalid UTF-8 in git diff output")?;
    
    // ãƒ‘ãƒƒãƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¤œè¨¼
    validate_patch_format(&patch)?;
    
    Ok(patch)
}

/// ãƒ‘ãƒƒãƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¤œè¨¼
pub fn validate_patch_format(patch: &str) -> Result<()> {
    // unified diffå½¢å¼ã®æ¤œè¨¼
    if !patch.starts_with("diff --git") {
        anyhow::bail!("Invalid patch format: must start with 'diff --git'");
    }
    
    // ãƒãƒ£ãƒ³ã‚¯å®šç¾©ã®å­˜åœ¨ç¢ºèª
    if !patch.contains("@@") {
        anyhow::bail!("Invalid patch format: missing chunk markers '@@'");
    }
    
    Ok(())
}

/// JSONå‡ºåŠ›
pub fn write_patches_json(
    patches: &[PatchOutput],
    output_path: &str,
) -> Result<()> {
    use std::fs::File;
    use std::io::BufWriter;
    
    let file = File::create(output_path)?;
    let writer = BufWriter::new(file);
    
    serde_json::to_writer_pretty(writer, patches)?;
    
    Ok(())
}
```

**æ¤œè¨¼é …ç›®**:
- [ ] unified diffå½¢å¼ã§å‡ºåŠ›
- [ ] ãƒ‘ãƒƒãƒãŒå…¬å¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æº–æ‹ 
- [ ] JSONå‡ºåŠ›ãŒæ­£ã—ã„
- [ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒé©åˆ‡

---

### 4. CLIã‚³ãƒãƒ³ãƒ‰å®Ÿè£…

```rust
// crates/miyabi-cli/src/commands/benchmark.rs

use clap::{Args, Subcommand};

#[derive(Args)]
pub struct BenchmarkArgs {
    #[command(subcommand)]
    pub command: BenchmarkCommand,
}

#[derive(Subcommand)]
pub enum BenchmarkCommand {
    /// SWE-bench Proè©•ä¾¡ã‚’å®Ÿè¡Œ
    Run {
        /// ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
        #[arg(long, default_value = "swebench_pro")]
        dataset: String,
        
        /// è©•ä¾¡ã™ã‚‹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ•°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        #[arg(long)]
        instances: Option<usize>,
        
        /// å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        #[arg(long, default_value = "results/patches.json")]
        output: String,
        
        /// ä¸¦åˆ—å®Ÿè¡Œæ•°
        #[arg(long, default_value = "5")]
        concurrency: usize,
        
        /// ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
        #[arg(long, default_value = "1800")]
        timeout: u64,
    },
    
    /// çµæœã‚’åˆ†æ
    Analyze {
        /// è©•ä¾¡çµæœãƒ•ã‚¡ã‚¤ãƒ«
        #[arg(long)]
        results: String,
    },
}
```

**ä½¿ç”¨ä¾‹**:
```bash
# ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆè©•ä¾¡ï¼ˆ10ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼‰
miyabi benchmark run --instances 10 --output results/pilot.json

# ãƒ•ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«è©•ä¾¡
miyabi benchmark run --output results/full.json --concurrency 5

# çµæœåˆ†æ
miyabi benchmark analyze --results results/full/evaluation_results.json
```

**æ¤œè¨¼é …ç›®**:
- [ ] CLIã‚³ãƒãƒ³ãƒ‰ãŒå‹•ä½œ
- [ ] å¼•æ•°ãŒæ­£ã—ãè§£æã•ã‚Œã‚‹
- [ ] ãƒ˜ãƒ«ãƒ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ˜ç¢º

---

## ğŸ“¦ æˆæœç‰©

- [ ] `crates/miyabi-benchmark/src/evaluator.rs` - è©•ä¾¡ãƒ©ãƒƒãƒ‘ãƒ¼
- [ ] `crates/miyabi-benchmark/src/patch.rs` - ãƒ‘ãƒƒãƒç”Ÿæˆ
- [ ] `crates/miyabi-benchmark/src/lib.rs` - ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
- [ ] `crates/miyabi-cli/src/commands/benchmark.rs` - CLIã‚³ãƒãƒ³ãƒ‰
- [ ] `crates/miyabi-benchmark/tests/evaluator_test.rs` - å˜ä½“ãƒ†ã‚¹ãƒˆ
- [ ] `crates/miyabi-benchmark/tests/patch_test.rs` - ãƒ‘ãƒƒãƒç”Ÿæˆãƒ†ã‚¹ãƒˆ

---

## âœ… æˆåŠŸåŸºæº–

### å¿…é”
- [ ] ãƒ‘ãƒƒãƒãŒå…¬å¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨å®Œå…¨ä¸€è‡´
- [ ] JSONå‡ºåŠ›ãŒå…¬å¼ã‚¹ã‚­ãƒ¼ãƒã«æº–æ‹ 
- [ ] 10ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§æ­£å¸¸å‹•ä½œ
- [ ] ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒæ­£ã—ãæ©Ÿèƒ½

### æ¨å¥¨
- [ ] ä¸¦åˆ—å®Ÿè¡ŒãŒå®‰å®šï¼ˆã‚¯ãƒ©ãƒƒã‚·ãƒ¥0ä»¶ï¼‰
- [ ] ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ˜ç¢º
- [ ] å˜ä½“ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸80%ä»¥ä¸Š

---

## â±ï¸ æœŸé–“

**æœŸé–“**: 2é€±é–“
**æœŸé™**: 2025-11-19

---

## ğŸ·ï¸ Label

- `âœ¨ type:feature`
- `ğŸ—ï¸ phase:implementation`
- `ğŸ¤– agent:codegen`
- `âš ï¸ priority:P1-High`

---

**è¦ªIssue**: #396 - SWE-bench Proè©•ä¾¡å®Ÿè£…
**ä¾å­˜**: #399 (Phase 2)

### 3. Add tests for #400

- **ID**: `task-400-test`
- **Type**: Test
- **Assigned Agent**: CodeGenAgent
- **Priority**: 2
- **Estimated Duration**: 15 min
- **Dependencies**: task-400-impl

**Description**: Create comprehensive test coverage

### 4. Review code quality for #400

- **ID**: `task-400-review`
- **Type**: Refactor
- **Assigned Agent**: ReviewAgent
- **Priority**: 3
- **Estimated Duration**: 10 min
- **Dependencies**: task-400-test

**Description**: Run quality checks and code review

## ğŸ”„ Execution Plan (DAG Levels)

Tasks can be executed in parallel within each level:

### Level 0 (Parallel Execution)

- `task-400-analysis` - Analyze requirements for #400

### Level 1 (Parallel Execution)

- `task-400-impl` - Implement solution for #400

### Level 2 (Parallel Execution)

- `task-400-test` - Add tests for #400

### Level 3 (Parallel Execution)

- `task-400-review` - Review code quality for #400

## ğŸ“Š Dependency Graph

```mermaid
graph TD
    task_400_analysis["Analyze requirements for #400"]
    task_400_impl["Implement solution for #400"]
    task_400_test["Add tests for #400"]
    task_400_review["Review code quality for #400"]
    task_400_analysis --> task_400_impl
    task_400_impl --> task_400_test
    task_400_test --> task_400_review
```

## â±ï¸ Timeline Estimation

- **Sequential Execution**: 60 minutes (1.0 hours)
- **Parallel Execution (Critical Path)**: 10 minutes (0.2 hours)
- **Estimated Speedup**: 6.0x

---

*Generated by CoordinatorAgent on 2025-11-01 06:15:52 UTC*
