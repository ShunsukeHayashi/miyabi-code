# Miyabi AWS Platform - Scalability & Extensibility Deep Dive

**Project**: Miyabi AWS Platform
**Version**: 1.0.0
**Date**: 2025-11-12
**Status**: Scalability Design Document

---

## ğŸ¯ Overview

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€Miyabi AWS Platformã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã¨æ‹¡å¼µæ€§ã«ã¤ã„ã¦ã€**å…·ä½“çš„ãªæ•°å€¤ã€ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã€è§£æ±ºç­–**ã‚’è©³ç´°ã«èª¬æ˜ã—ã¾ã™ã€‚

---

## ğŸ“Š Scalability Dimensions - 5ã¤ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è»¸

### 1. Horizontal Scaling (æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°)

**å®šç¾©**: åŒã˜ç¨®é¡ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å¢—ã‚„ã™ã“ã¨ã§å‡¦ç†èƒ½åŠ›ã‚’å‘ä¸Š

#### 1.1 ECS Fargate Agent Workers

**ç¾åœ¨ã®è¨­è¨ˆ**:
```
Min Tasks: 1
Max Tasks: 100
Target CPU: 70%
```

**è©³ç´°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ **:

```
Queue Depth (SQS)          Auto-Scaling Action
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0-10 messages         â†’    Desired Count: 1
11-50 messages        â†’    Desired Count: 5
51-100 messages       â†’    Desired Count: 10
101-500 messages      â†’    Desired Count: 20
501-1000 messages     â†’    Desired Count: 50
1000+ messages        â†’    Desired Count: 100
```

**ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é€Ÿåº¦**:
- **Scale Out**: 60ç§’ä»¥å†…ã«æ–°ã—ã„ã‚¿ã‚¹ã‚¯ãŒèµ·å‹•
- **Scale In**: 300ç§’ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³å¾Œã«ç¸®å°ï¼ˆå®‰å…¨æ€§é‡è¦–ï¼‰

**å®Ÿéš›ã®å‡¦ç†èƒ½åŠ›**:
```
1 Worker:
  - 1 Issueå‡¦ç†/15åˆ† = 4 Issues/hour
  - 1æ—¥ç¨¼åƒ: 96 Issues/day

10 Workers (ä¸¦åˆ—):
  - 10 Issues/15åˆ† = 40 Issues/hour
  - 1æ—¥ç¨¼åƒ: 960 Issues/day

100 Workers (æœ€å¤§):
  - 100 Issues/15åˆ† = 400 Issues/hour
  - 1æ—¥ç¨¼åƒ: 9,600 Issues/day
```

**ã‚³ã‚¹ãƒˆè¨ˆç®—**:
```
1 Worker:
  - vCPU: 0.5
  - Memory: 1GB
  - Cost: $0.04048/hour (us-east-1)

10 Workers (24/7):
  - Cost: $0.4048/hour Ã— 24 Ã— 30 = $291.46/month

100 Workers (ãƒ”ãƒ¼ã‚¯æ™‚ã®ã¿ã€1æ—¥4æ™‚é–“):
  - Cost: $4.048/hour Ã— 4 Ã— 30 = $485.76/month
```

**ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**:
1. **EFS I/O**: å…±æœ‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆWorktreesï¼‰ã®åŒæ™‚ã‚¢ã‚¯ã‚»ã‚¹
   - **è§£æ±ºç­–**: EFS Provisioned Throughput + ä¸¦åˆ—åº¦åˆ¶å¾¡
2. **DynamoDB Write Capacity**: çŠ¶æ…‹æ›´æ–°ã®æ›¸ãè¾¼ã¿
   - **è§£æ±ºç­–**: On-Demand Billing Modeï¼ˆè‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
3. **GitHub API Rate Limit**: 5000 requests/hour
   - **è§£æ±ºç­–**: Token rotation + Caching + Exponential backoff

---

#### 1.2 Lambda Function Concurrency

**ç¾åœ¨ã®è¨­è¨ˆ**:
```
Reserved Concurrency: 100 per function
Provisioned Concurrency: 10 (warm instances)
```

**è©³ç´°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ **:

```
Request Rate               Lambda Scaling
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
< 10 req/sec          â†’    10 instances (warm)
10-100 req/sec        â†’    Scale to 100 instances
100-500 req/sec       â†’    Queue + throttle
> 500 req/sec         â†’    API Gateway throttling
```

**ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆæœ€é©åŒ–**:
```
Without optimization:
  - Cold start: 3-5 seconds
  - P95 latency: 4.2 seconds

With Provisioned Concurrency (10 instances):
  - Warm start: 50-100ms
  - P95 latency: 150ms
  - Cost increase: ~$100/month
```

**å®Ÿéš›ã®å‡¦ç†èƒ½åŠ›**:
```
API Gateway + Lambda:
  - 1 Lambda: 1000 req/sec (burst)
  - 100 Lambdas: 100,000 req/sec (theoretical)
  - Realistic: 10,000 req/sec (API Gateway limit)
```

---

#### 1.3 Database Scaling

**DynamoDB**:
```
Mode: On-Demand (Pay per request)

Read Capacity:
  - Auto-scales: 0 â†’ Unlimited
  - Latency: < 10ms (P99)
  - Cost: $0.25 per million reads

Write Capacity:
  - Auto-scales: 0 â†’ Unlimited
  - Latency: < 10ms (P99)
  - Cost: $1.25 per million writes

Burst Capacity:
  - 2Ã— previous peak for 30 minutes
  - Allows sudden spikes
```

**RDS Aurora Serverless v2**:
```
ACU (Aurora Capacity Units):
  - Min: 0.5 ACU (1GB RAM, ~1 vCPU)
  - Max: 128 ACU (256GB RAM, ~64 vCPU)
  - Scaling: Granular (0.5 ACU increments)
  - Response: < 1 second

Cost:
  - 0.5 ACU: $0.12/hour = $87.60/month
  - 1 ACU: $0.24/hour = $175.20/month
  - 4 ACU: $0.96/hour = $700.80/month

Connection Pooling:
  - RDS Proxy: 1000+ concurrent connections
  - Reduces connection overhead
  - Cost: $0.015/hour = $10.80/month
```

**å®Ÿéš›ã®å‡¦ç†èƒ½åŠ›**:
```
0.5 ACU (Idle):
  - ~500 queries/sec (simple SELECT)
  - ~50 queries/sec (complex JOIN)

4 ACU (Medium load):
  - ~4000 queries/sec (simple SELECT)
  - ~400 queries/sec (complex JOIN)

16 ACU (High load):
  - ~16,000 queries/sec (simple SELECT)
  - ~1600 queries/sec (complex JOIN)
```

---

### 2. Vertical Scaling (å‚ç›´ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°)

**å®šç¾©**: å€‹ã€…ã®ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¹ãƒšãƒƒã‚¯ã‚’å‘ä¸Š

#### 2.1 ECS Task Definition Sizing

**Agent Worker ã‚¿ã‚¹ã‚¯**:

| Workload Type | vCPU | Memory | Use Case |
|--------------|------|--------|----------|
| **Light** | 0.25 | 512MB | Issueç®¡ç†ã€ãƒ©ãƒ™ãƒ«æ¨è«– |
| **Standard** | 0.5 | 1GB | CodeGenã€Reviewï¼ˆç¾åœ¨ã®è¨­å®šï¼‰ |
| **Medium** | 1 | 2GB | ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã€è¤‡é›‘ãªãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ |
| **Heavy** | 2 | 4GB | å¤§è¦æ¨¡IaCç”Ÿæˆã€è¤‡æ•°ãƒªãƒ¼ã‚¸ãƒ§ãƒ³å±•é–‹ |
| **XL** | 4 | 8GB | ãƒãƒ«ãƒã‚¢ã‚«ã‚¦ãƒ³ãƒˆç®¡ç†ã€ä¸€æ‹¬æœ€é©åŒ– |

**å®Ÿéš›ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**:
```
Light (0.25 vCPU, 512MB):
  - Issueå‡¦ç†: ~5åˆ†
  - åŒæ™‚å®Ÿè¡Œ: 200 tasks (cost-effective)
  - Cost: $14.54/hour (200 tasks Ã— 24/7)

Standard (0.5 vCPU, 1GB):
  - Issueå‡¦ç†: ~15åˆ† (ç¾åœ¨)
  - åŒæ™‚å®Ÿè¡Œ: 100 tasks
  - Cost: $29.10/hour (100 tasks Ã— 24/7)

Heavy (2 vCPU, 4GB):
  - Multi-region deploy: ~30åˆ†
  - åŒæ™‚å®Ÿè¡Œ: 25 tasks
  - Cost: $58.20/hour (25 tasks Ã— 24/7)
```

**å‹•çš„ã‚¿ã‚¹ã‚¯ã‚µã‚¤ã‚¸ãƒ³ã‚°**:
```rust
fn select_task_size(issue: &Issue) -> TaskDefinition {
    match (issue.labels, issue.complexity) {
        // Light tasks
        (["label"], _) => TaskDefinition::Light,

        // Standard tasks
        (["feature"], Low) => TaskDefinition::Standard,

        // Heavy tasks
        (["aws", "multi-region"], _) => TaskDefinition::Heavy,
        (_, High) => TaskDefinition::Heavy,

        _ => TaskDefinition::Standard
    }
}
```

---

#### 2.2 Lambda Memory Configuration

**Memory vs Performance**:

| Memory | vCPU | Duration (avg) | Cost/invocation | Use Case |
|--------|------|----------------|-----------------|----------|
| 128MB | 0.08 | 2000ms | $0.0000004 | Webhookå—ä¿¡ |
| 512MB | 0.33 | 500ms | $0.0000004 | è»½é‡API |
| 1024MB | 0.67 | 250ms | $0.0000004 | æ¨™æº–API |
| 3008MB | 2.00 | 80ms | $0.0000005 | é‡ã„å‡¦ç† |

**æœ€é©ãƒ¡ãƒ¢ãƒªã®è¨ˆç®—**:
```
Cost = (Memory/1024) Ã— Duration Ã— $0.0000166667

Example:
- 512MB, 500ms: $0.00000416
- 1024MB, 250ms: $0.00000416 (åŒã˜ã‚³ã‚¹ãƒˆã€2å€é€Ÿã„ï¼)

â†’ ãƒ¡ãƒ¢ãƒªã‚’å¢—ã‚„ã™ã¨å‡¦ç†æ™‚é–“çŸ­ç¸®ã§ã‚³ã‚¹ãƒˆã¯åŒã˜ã¾ãŸã¯å®‰ããªã‚‹
```

**Power Tuning**:
```bash
# AWS Lambda Power Tuning ãƒ„ãƒ¼ãƒ«ä½¿ç”¨
aws lambda invoke \
  --function-name my-function \
  --payload '{"powerValues": [128,256,512,1024,1536,3008]}' \
  --output json

# çµæœä¾‹:
Optimal: 1024MB (æœ€é€Ÿ & æœ€å®‰)
```

---

### 3. Auto-Scaling Policies (è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°)

#### 3.1 Target Tracking Scaling

**ECS Service - CPU Based**:
```yaml
TargetTrackingScalingPolicy:
  TargetValue: 70.0
  ScaleInCooldown: 300    # 5åˆ†å¾…ã£ã¦ã‹ã‚‰ã‚¹ã‚±ãƒ¼ãƒ«ã‚¤ãƒ³
  ScaleOutCooldown: 60    # 1åˆ†ã§å³åº§ã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ

  Metric: ECSServiceAverageCPUUtilization
```

**å®Ÿéš›ã®æŒ™å‹•**:
```
Time    CPU%    Desired    Actual    Action
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
00:00   50%     10         10        (Stable)
00:05   75%     10         10        CPU > 70% detected
00:06   75%     15         10        Scale out triggered
00:07   75%     15         15        New tasks running
00:10   60%     15         15        (Stable)
00:15   50%     15         15        (Cooldown)
00:20   50%     10         15        Scale in triggered
00:25   50%     10         10        Tasks terminated
```

---

#### 3.2 Step Scaling (Queue Depth)

**SQS Queue Depth Based**:
```yaml
StepScalingPolicy:
  Metric: ApproximateNumberOfMessagesVisible

  Steps:
    - MetricIntervalLowerBound: 0
      MetricIntervalUpperBound: 10
      ScalingAdjustment: -1      # æ¸›ã‚‰ã™

    - MetricIntervalLowerBound: 10
      MetricIntervalUpperBound: 50
      ScalingAdjustment: +5

    - MetricIntervalLowerBound: 50
      MetricIntervalUpperBound: 100
      ScalingAdjustment: +10

    - MetricIntervalLowerBound: 100
      MetricIntervalUpperBound: null
      ScalingAdjustment: +20
```

**å®Ÿéš›ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**:
```
Time    Queue   Desired    Actual    Action
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
00:00   5       1          1         (Idle)
00:01   15      1          1         Step triggered
00:02   15      6          1         +5 tasks
00:03   60      6          6         Step triggered
00:04   60      16         6         +10 tasks
00:05   120     16         16        Step triggered
00:06   120     36         16        +20 tasks
00:07   50      36         36        (Draining)
00:10   5       36         36        (Cooldown)
00:15   5       1          36        Scale in
00:20   5       1          1         (Idle)
```

---

#### 3.3 Scheduled Scaling

**äºˆæ¸¬å¯èƒ½ãªè² è·ãƒ‘ã‚¿ãƒ¼ãƒ³**:
```yaml
ScheduledActions:
  # å¹³æ—¥æœã®ãƒ”ãƒ¼ã‚¯å¯¾å¿œï¼ˆæ—¥æœ¬æ™‚é–“9:00 = UTC 0:00ï¼‰
  - Name: MorningRampUp
    Schedule: "cron(0 0 * * MON-FRI)"
    MinCapacity: 20
    MaxCapacity: 100

  # å¤œé–“ã®ç¸®å°ï¼ˆæ—¥æœ¬æ™‚é–“22:00 = UTC 13:00ï¼‰
  - Name: NightScaleDown
    Schedule: "cron(0 13 * * *)"
    MinCapacity: 1
    MaxCapacity: 10
```

**ã‚³ã‚¹ãƒˆå‰Šæ¸›åŠ¹æœ**:
```
Without Scheduled Scaling:
  - 24/7 with 10 min tasks
  - Cost: $291.46/month

With Scheduled Scaling:
  - Business hours (9:00-22:00, 13h): 20 tasks avg
  - Night hours (22:00-9:00, 11h): 2 tasks avg
  - Cost: (20Ã—13 + 2Ã—11) Ã— 30 Ã— $0.04048
       = (260 + 22) Ã— 30 Ã— $0.04048
       = $342.45/month... (é€†ã«é«˜ã„ï¼)

â†’ Scheduled Scaling ã¯é•·æ™‚é–“ãƒ”ãƒ¼ã‚¯ãŒã‚ã‚‹å ´åˆã®ã¿æœ‰åŠ¹
â†’ Miyabiã®å ´åˆã€Queue-basedãŒæœ€é©
```

---

### 4. Data Partitioning & Sharding (ãƒ‡ãƒ¼ã‚¿åˆ†å‰²)

#### 4.1 DynamoDB Partition Key Strategy

**ç¾åœ¨ã®è¨­è¨ˆ**:
```
PK: "TASK#{issue_number}"
SK: "METADATA" | "EXECUTION#{phase}" | "LOG#{timestamp}"
```

**ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**:

**âŒ Bad Partition Key**:
```
PK: "TASK"  # å…¨ã¦ã®ã‚¿ã‚¹ã‚¯ãŒåŒã˜ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«é›†ä¸­
â†’ Hot Partitionå•é¡Œ
â†’ Throughput: ~3000 RCU/sec per partition (åˆ¶é™)
```

**âœ… Good Partition Key**:
```
PK: "TASK#{issue_number}"  # å„IssueãŒç‹¬ç«‹ã—ãŸãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
â†’ Uniform distribution
â†’ Throughput: Unlimited (ç†è«–ä¸Š)
```

**ã•ã‚‰ã«æœ€é©åŒ–ï¼ˆå¤§è¦æ¨¡æ™‚ï¼‰**:
```
PK: "TASK#{issue_number % 100}#{issue_number}"
     â””â”€â”€ 0-99ã®ã‚·ãƒ£ãƒ¼ãƒ‰ID

Example:
  Issue #12345 â†’ PK: "TASK#45#12345"
  Issue #12346 â†’ PK: "TASK#46#12346"

â†’ è² è·ã‚’100å€‹ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«å‡ç­‰åˆ†æ•£
```

**å®Ÿéš›ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**:
```
Without sharding (1 partition):
  - Max: 3000 RCU/sec, 1000 WCU/sec
  - åŒæ™‚å‡¦ç†: ~100 tasks

With sharding (100 partitions):
  - Max: 300,000 RCU/sec, 100,000 WCU/sec
  - åŒæ™‚å‡¦ç†: ~10,000 tasks
```

---

#### 4.2 RDS Aurora Read Replicas

**ç¾åœ¨ã®è¨­è¨ˆ**:
```
1 Writer + 0 Readers
```

**æ‹¡å¼µè¨­è¨ˆ**:
```
1 Writer + 5 Readers (Max: 15 readers)

Read Replica Distribution:
  - Reader 1: Agent queries (high load)
  - Reader 2: Knowledge queries
  - Reader 3: Analytics queries
  - Reader 4: Monitoring queries
  - Reader 5: Backup queries

Routing:
  - Write: Primary endpoint
  - Read: Reader endpoint (round-robin)
```

**å®Ÿéš›ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**:
```
1 Writer only:
  - Reads: 4000 queries/sec
  - Writes: 1000 queries/sec
  - Total: 5000 queries/sec

1 Writer + 5 Readers:
  - Reads: 20,000 queries/sec (5Ã— readers)
  - Writes: 1000 queries/sec
  - Total: 21,000 queries/sec

Cost increase: 5Ã— reader = +$438/month (from $87.60)
```

**æ¥ç¶šãƒ—ãƒ¼ãƒ«æœ€é©åŒ–**:
```rust
// RDS Proxyä½¿ç”¨
use aws_sdk_rds::Client;

let pool_config = PoolConfig {
    max_connections: 100,        // Writer: 100
    max_reader_connections: 500, // Readers: 500 (5Ã— 100)
    connection_timeout: Duration::from_secs(30),
};

// Read/Writeåˆ†é›¢
async fn execute_query(query: &str, read_only: bool) -> Result<Rows> {
    let endpoint = if read_only {
        "reader-endpoint"  // Round-robin across 5 readers
    } else {
        "writer-endpoint"  // Primary only
    };

    client.query(endpoint, query).await
}
```

---

### 5. Caching Strategies (ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°)

#### 5.1 Multi-Layer Caching

**Layer 1: CloudFront (Edge Cache)**:
```
TTL: 3600s (1 hour)
Cacheable:
  - Static assets (HTML, CSS, JS, images)
  - API responses (with Cache-Control header)

Cache Hit Rate: 85-90%
Cost Savings: ~70% (origin requests)

Example:
  Without cache: 1M requests/month
    â†’ CloudFront: $8.50
    â†’ Origin (Lambda): $20.00
    â†’ Total: $28.50

  With cache (90% hit):
    â†’ CloudFront: $8.50
    â†’ Origin (100K): $2.00
    â†’ Total: $10.50 (63% savings)
```

**Layer 2: API Gateway Cache**:
```
Cache Size: 0.5GB (small), 6.1GB (medium), 13.5GB (large)
TTL: 300s (5 minutes)
Cost: $0.02/hour/GB = ~$15/month (0.5GB)

Cacheable Endpoints:
  GET /api/v1/agents         â†’ Cache 5min
  GET /api/v1/tasks/:id      â†’ Cache 30sec
  GET /api/v1/knowledge      â†’ Cache 1min
```

**Layer 3: Application Cache (Redis/ElastiCache)**:
```
Node Type: cache.t3.micro
Cost: $12.41/month

Use Cases:
  - Session data (Cognito tokens)
  - Rate limiting counters
  - Hot data (frequently accessed tasks)

TTL Strategy:
  - Session: 24 hours
  - Rate limit: 1 hour
  - Hot data: 5 minutes
```

**Layer 4: Database Query Cache (Aurora)**:
```
Built-in Aurora Query Cache
Size: Configurable (10% of memory)

Automatically caches:
  - Identical SELECT queries
  - Result sets

Cache invalidation:
  - Automatic on writes to related tables
```

**å®Ÿéš›ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**:
```
Without caching:
  - API latency: 200ms (Lambda + DB)
  - Cost: $50/month (1M requests)

With 4-layer caching (90% hit):
  - API latency: 10ms (CloudFront edge)
  - Cost: $25/month (100K origin requests + cache cost)
  - 50% cost reduction, 20Ã— faster
```

---

### 6. Asynchronous Processing (éåŒæœŸå‡¦ç†)

#### 6.1 Event-Driven Architecture

**ç¾åœ¨ã®è¨­è¨ˆ**:
```
Synchronous (Bad):
  User Request â†’ API Gateway â†’ Lambda â†’ [Wait 15min] â†’ Response

Asynchronous (Good):
  User Request â†’ API Gateway â†’ Lambda â†’ SQS â†’ Response (202 Accepted)
                                          â†“
                                     ECS Worker (background)
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
```
Synchronous:
  - API Gateway timeout: 29 seconds (hard limit)
  - Lambda timeout: 15 minutes (hard limit)
  - User waits: 15 minutes
  - Scalability: Limited by Lambda concurrency

Asynchronous:
  - API response: < 100ms (just enqueue)
  - User doesn't wait
  - Worker timeout: Unlimited (ECS)
  - Scalability: Unlimited (SQS + ECS)
```

---

#### 6.2 Message Queue Patterns

**Dead Letter Queue (DLQ)**:
```yaml
MainQueue:
  MaxReceiveCount: 3    # 3å›å¤±æ•—ã—ãŸã‚‰DLQã¸
  VisibilityTimeout: 3600s (1 hour)

DeadLetterQueue:
  MessageRetentionPeriod: 1209600s (14 days)
  AlarmOnMessage: true
```

**Priority Queues**:
```
High Priority Queue (Critical Issues):
  - P0, P1 issues
  - Security vulnerabilities
  - Worker poll interval: 10s

Standard Queue (Normal Issues):
  - P2, P3 issues
  - Feature requests
  - Worker poll interval: 30s

Low Priority Queue (Background):
  - Documentation updates
  - Cleanup tasks
  - Worker poll interval: 300s
```

**Batch Processing**:
```rust
// SQS Long Polling (20s)
let messages = sqs_client
    .receive_message()
    .queue_url(queue_url)
    .max_number_of_messages(10)    // Batch size
    .wait_time_seconds(20)          // Long polling
    .send()
    .await?;

// Process in parallel
let tasks: Vec<_> = messages
    .messages
    .unwrap()
    .into_iter()
    .map(|msg| tokio::spawn(process_message(msg)))
    .collect();

futures::future::join_all(tasks).await;
```

**å®Ÿéš›ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**:
```
Short Polling (0s):
  - 1000 requests/sec
  - Cost: $4.00/month (1B requests)
  - Empty responses: 99.9% (wasteful)

Long Polling (20s):
  - 50 requests/sec
  - Cost: $0.20/month (50M requests)
  - Empty responses: 0.1%
  - 95% cost reduction
```

---

### 7. Multi-Region Architecture (ãƒãƒ«ãƒãƒªãƒ¼ã‚¸ãƒ§ãƒ³)

#### 7.1 Active-Active Configuration

**ç¾åœ¨ã®è¨­è¨ˆ (Single Region)**:
```
us-east-1 (Primary)
  â”œâ”€ All traffic (100%)
  â””â”€ Single Point of Failure
```

**æ‹¡å¼µè¨­è¨ˆ (Multi-Region)**:
```
us-east-1 (Primary)          ap-northeast-1 (Secondary)
  â”œâ”€ 100% traffic (normal)      â”œâ”€ 0% traffic (normal)
  â”œâ”€ ECS: 10-100 tasks          â”œâ”€ ECS: 5-50 tasks
  â”œâ”€ DynamoDB: Global Table     â”œâ”€ DynamoDB: Replica
  â””â”€ Aurora: Writer             â””â”€ Aurora: Reader

Route 53 Health Check:
  - Primary healthy â†’ 100% to us-east-1
  - Primary failed â†’ 100% to ap-northeast-1 (failover)
  - Failover time: < 1 minute
```

**DynamoDB Global Tables**:
```
Replication:
  - Multi-region, multi-master
  - Eventual consistency (< 1 second)
  - Conflict resolution: Last-writer-wins

Cost:
  - Replicated write: 2Ã— cost ($2.50/M writes)
  - Cross-region transfer: $0.02/GB
  - Total increase: ~30%
```

**Aurora Global Database**:
```
Primary Region (us-east-1):
  - 1 Writer + 2 Readers
  - Replication lag: < 1 second

Secondary Region (ap-northeast-1):
  - Read-only replicas (up to 5)
  - Promote to Writer: < 1 minute (failover)

Cost:
  - Secondary region: +100% (full cluster)
  - Total: $175/month â†’ $350/month
```

**å®Ÿéš›ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**:
```
Single Region (us-east-1):
  - New York: 10ms
  - Tokyo: 150ms

Multi-Region (us-east-1 + ap-northeast-1):
  - New York â†’ us-east-1: 10ms
  - Tokyo â†’ ap-northeast-1: 10ms
  - 15Ã— faster for APAC users
```

---

## ğŸ”¥ Bottlenecks & Solutions - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨è§£æ±ºç­–

### Bottleneck 1: EFS I/O Performance

**å•é¡Œ**:
```
Worktree operations (git clone, checkout, commit):
  - 100 concurrent tasks
  - Each task: 500 MB repository
  - Total I/O: 50 GB simultaneous

EFS Standard Performance:
  - Baseline: 50 MB/s per TB stored
  - Burst: 100 MB/s (credit-based)
  - 50 GB / 100 MB/s = 500 seconds (8åˆ†!)
```

**è§£æ±ºç­–**:
```yaml
EFS Provisioned Throughput:
  - Provision: 500 MB/s
  - Cost: $6.00/MB/s/month = $3000/month (expensive!)

Alternative: EBS volumes per task
  - Each task: 20GB EBS gp3
  - Performance: 3000 IOPS, 125 MB/s
  - Cost: $1.60/month per task
  - 100 tasks: $160/month (much cheaper!)
```

**å®Ÿè£…**:
```typescript
// CDK: EBS volume per task
const taskDef = new FargateTaskDefinition(this, 'WorkerTask', {
  volumes: [{
    name: 'worktree-volume',
    efsVolumeConfiguration: undefined,  // Don't use EFS
  }]
});

// Use ephemeral storage (Docker volume)
taskDef.addContainer('Worker', {
  image: ContainerImage.fromRegistry('miyabi-worker:latest'),
  // Each task gets 20GB ephemeral storage (free!)
  ephemeralStorageGiB: 20,
});
```

---

### Bottleneck 2: GitHub API Rate Limit

**å•é¡Œ**:
```
GitHub API Rate Limit:
  - 5000 requests/hour per token
  - 100 workers Ã— 50 API calls = 5000 calls (limit reached!)
```

**è§£æ±ºç­–**:

**1. Token Rotation**:
```rust
struct GitHubClientPool {
    tokens: Vec<String>,
    current: AtomicUsize,
}

impl GitHubClientPool {
    async fn get_client(&self) -> GitHubClient {
        let index = self.current.fetch_add(1, Ordering::SeqCst) % self.tokens.len();
        GitHubClient::new(&self.tokens[index])
    }
}

// 10 tokens = 50,000 requests/hour
let pool = GitHubClientPool::new(vec![
    token1, token2, token3, token4, token5,
    token6, token7, token8, token9, token10,
]);
```

**2. Conditional Requests (ETags)**:
```rust
// Cache with ETags
let (data, etag) = client.get_issue(123).await?;
cache.set(123, (data.clone(), etag.clone()));

// Later request
if let Some((cached_data, cached_etag)) = cache.get(&123) {
    match client.get_issue_if_modified(123, &cached_etag).await? {
        Response::NotModified => return Ok(cached_data),
        Response::Modified(new_data, new_etag) => {
            cache.set(123, (new_data.clone(), new_etag));
            return Ok(new_data);
        }
    }
}

// Saves ~50% API calls (for unchanged resources)
```

**3. GraphQL (vs REST)**:
```graphql
# 1 GraphQL request = multiple REST requests
query {
  repository(owner: "org", name: "repo") {
    issue(number: 123) {
      title
      body
      labels { name }
      assignees { login }
      comments(first: 10) {
        nodes { body }
      }
    }
  }
}

# REST equivalent: 3 requests (issue, labels, comments)
# GraphQL: 1 request
# Savings: 67%
```

---

### Bottleneck 3: Database Connection Pool

**å•é¡Œ**:
```
RDS Aurora:
  - Max connections: 1000 (t3.medium)
  - Each Lambda: 1 connection
  - 1000 concurrent Lambdas = connection exhausted
  - New Lambda: Connection refused!
```

**è§£æ±ºç­–**:

**1. RDS Proxy**:
```typescript
const dbProxy = new DatabaseProxy(this, 'DBProxy', {
  proxyTarget: ProxyTarget.fromCluster(dbCluster),
  secrets: [dbCredentials],
  vpc,
  maxConnectionsPercent: 100,  // Use all connections
  maxIdleConnectionsPercent: 50,  // Reclaim idle
  connectionBorrowTimeout: Duration.seconds(30),
});

// Lambda uses Proxy instead of direct connection
lambda.addEnvironment('DB_ENDPOINT', dbProxy.endpoint);
```

**Benefits**:
```
Without RDS Proxy:
  - Max concurrent: 1000 Lambdas
  - Connection time: 100ms (new connection)
  - Failure rate: 10% (connection refused)

With RDS Proxy:
  - Max concurrent: 10,000+ Lambdas
  - Connection time: 10ms (pooled)
  - Failure rate: 0.1%
  - 10Ã— scalability, 10Ã— faster
```

**2. Connection Pooling in Rust**:
```rust
use deadpool_postgres::{Config, Pool, Runtime};

// Create pool once (application startup)
let mut cfg = Config::new();
cfg.host = Some(db_host.to_string());
cfg.dbname = Some("miyabi".to_string());
cfg.manager = Some(ManagerConfig {
    recycling_method: RecyclingMethod::Fast,
});

let pool = cfg.create_pool(Some(Runtime::Tokio1), NoTls)?;

// Reuse connections
async fn query(pool: &Pool) -> Result<Vec<Row>> {
    let client = pool.get().await?;  // Get from pool
    let rows = client.query("SELECT * FROM tasks", &[]).await?;
    Ok(rows)  // Connection automatically returned to pool
}
```

---

### Bottleneck 4: Lambda Cold Start

**å•é¡Œ**:
```
Cold Start:
  - Lambda initialization: 1-3 seconds
  - First request: 3-5 seconds total latency
  - User experience: Poor
```

**è§£æ±ºç­–**:

**1. Provisioned Concurrency**:
```typescript
const apiFunction = new Function(this, 'ApiFunction', {
  runtime: Runtime.RUST_PROVIDED_AL2,
  code: Code.fromAsset('lambda/'),
  currentVersionOptions: {
    provisionedConcurrentExecutions: 10,  // Keep 10 warm
  },
});

// Cost: $0.015/hour Ã— 10 = $0.15/hour = $108/month
// Benefit: 50Ã— faster (3000ms â†’ 60ms)
```

**2. Lambda SnapStart (Java only)**:
```typescript
// For Java runtimes
const javaFunction = new Function(this, 'JavaFunction', {
  runtime: Runtime.JAVA_11,
  snapStart: lambda.SnapStartConf.ON_PUBLISHED_VERSIONS,
});

// Reduces cold start: 10 seconds â†’ 1 second
```

**3. Reduce Package Size (Rust)**:
```toml
# Cargo.toml
[profile.release]
opt-level = "z"      # Optimize for size
lto = true           # Link-time optimization
codegen-units = 1    # Single codegen unit
strip = true         # Strip debug symbols

# Binary size: 50MB â†’ 5MB (10Ã— smaller)
# Cold start: 3000ms â†’ 500ms (6Ã— faster)
```

**4. Warm-up Lambda**:
```typescript
// CloudWatch Events (every 5 minutes)
const rule = new Rule(this, 'WarmUpRule', {
  schedule: Schedule.rate(Duration.minutes(5)),
});

rule.addTarget(new targets.LambdaFunction(apiFunction, {
  event: RuleTargetInput.fromObject({ warmup: true }),
}));

// Lambda handler
if event.get("warmup"):
    return {"statusCode": 200, "body": "warmed"}
```

---

## ğŸ“ˆ Performance Benchmarks - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### Scenario 1: Low Load (10 Issues/hour)

```
Configuration:
  - ECS Workers: 1 task
  - Lambda: Provisioned 2 instances
  - DynamoDB: On-Demand
  - RDS: 0.5 ACU

Performance:
  - Issue processing: 15 minutes/issue
  - Throughput: 4 issues/hour
  - Latency: P95 200ms (API)

Cost:
  - ECS: $29.10/month
  - Lambda: $20/month
  - DynamoDB: $5/month
  - RDS: $87.60/month
  - Total: $141.70/month
```

---

### Scenario 2: Medium Load (100 Issues/hour)

```
Configuration:
  - ECS Workers: 10 tasks (auto-scaled)
  - Lambda: Provisioned 5 instances
  - DynamoDB: On-Demand
  - RDS: 2 ACU (auto-scaled)

Performance:
  - Issue processing: 15 minutes/issue
  - Throughput: 40 issues/hour
  - Latency: P95 150ms (API)

Cost:
  - ECS: $291.46/month (10Ã— workers)
  - Lambda: $50/month (5Ã— provisioned)
  - DynamoDB: $30/month
  - RDS: $350.40/month (2 ACU)
  - Total: $721.86/month
```

---

### Scenario 3: High Load (1000 Issues/hour)

```
Configuration:
  - ECS Workers: 100 tasks (auto-scaled)
  - Lambda: Reserved 50 instances
  - DynamoDB: On-Demand (high throughput)
  - RDS: 8 ACU + 3 Read Replicas

Performance:
  - Issue processing: 15 minutes/issue
  - Throughput: 400 issues/hour
  - Latency: P95 100ms (API)

Cost:
  - ECS: $2,914.60/month (100Ã— workers)
  - Lambda: $500/month (50Ã— reserved)
  - DynamoDB: $300/month
  - RDS: $2,803.20/month (8 ACU + replicas)
  - CloudFront: $50/month
  - Total: $6,567.80/month

Cost per Issue: $6,567.80 / (400Ã—24Ã—30) = $0.23/issue
```

---

### Scenario 4: Extreme Load (10,000 Issues/hour)

```
Configuration:
  - ECS Workers: 100 tasks (limit)
  - Lambda: Reserved 100 instances
  - DynamoDB: On-Demand (very high throughput)
  - RDS: 32 ACU + 10 Read Replicas
  - Multi-Region (us-east-1 + ap-northeast-1)

Performance:
  - Issue processing: 15 minutes/issue (queue builds up)
  - Throughput: 400 issues/hour (ECS limit reached)
  - Queue time: ~24 hours (backlog)
  - Latency: P95 50ms (API, cached)

Bottleneck: ECS task limit (100)

Solution: Increase max tasks to 500
  - ECS: 500 tasks
  - Throughput: 2000 issues/hour
  - Queue time: ~5 hours

Cost:
  - ECS: $14,573.00/month (500Ã— workers)
  - Lambda: $1,000/month
  - DynamoDB: $1,500/month
  - RDS: $11,212.80/month (32 ACU Ã— 2 regions)
  - CloudFront: $500/month
  - Total: $28,785.80/month

Cost per Issue: $28,785.80 / (2000Ã—24Ã—30) = $0.20/issue (cheaper!)
```

---

## ğŸ¯ Scalability Recommendations

### Phase 1: Launch (0-100 Issues/day)
```
âœ… Use: Single region, minimal resources
âœ… Cost: ~$150/month
âœ… Max: 100 issues/day
âŒ Don't: Over-provision resources
```

### Phase 2: Growth (100-1000 Issues/day)
```
âœ… Enable: Auto-scaling (10-50 tasks)
âœ… Add: Read replicas (RDS)
âœ… Implement: Multi-layer caching
âœ… Cost: ~$700/month
âœ… Max: 1000 issues/day
```

### Phase 3: Scale (1000-10,000 Issues/day)
```
âœ… Enable: Multi-region (failover)
âœ… Increase: Max tasks to 100-200
âœ… Add: RDS Proxy
âœ… Optimize: Token rotation, GraphQL
âœ… Cost: ~$7,000/month
âœ… Max: 10,000 issues/day
```

### Phase 4: Hyper-scale (10,000+ Issues/day)
```
âœ… Enable: Multi-region Active-Active
âœ… Increase: Max tasks to 500+
âœ… Implement: DynamoDB Global Tables
âœ… Add: ElastiCache
âœ… Consider: Lambda â†’ ECS for API (no cold start)
âœ… Cost: ~$30,000/month
âœ… Max: 100,000+ issues/day
```

---

## ğŸ“ Summary

**Key Takeaways**:

1. **Horizontal Scaling**: ECS tasks 1 â†’ 100 â†’ 500 (linear scaling)
2. **Vertical Scaling**: å‹•çš„ã‚¿ã‚¹ã‚¯ã‚µã‚¤ã‚¸ãƒ³ã‚°ï¼ˆlight/standard/heavyï¼‰
3. **Auto-Scaling**: Queue-based step scalingï¼ˆæœ€é©ï¼‰
4. **Caching**: 4-layer cachingï¼ˆ90% hit rateï¼‰
5. **Async Processing**: Event-driven architectureï¼ˆå¿…é ˆï¼‰
6. **Multi-Region**: Active-Passive â†’ Active-Activeï¼ˆãƒ•ã‚§ãƒ¼ã‚º3ä»¥é™ï¼‰

**Cost vs Performance**:
```
$150/month   â†’  100 issues/day   (launch)
$700/month   â†’  1,000 issues/day  (growth)
$7,000/month â†’  10,000 issues/day (scale)
$30,000/month â†’ 100,000 issues/day (hyper-scale)
```

**ãƒœãƒˆãƒ«ãƒãƒƒã‚¯å„ªå…ˆåº¦**:
1. ğŸ”´ **High**: EFS I/O â†’ EBS ephemeral storage
2. ğŸŸ  **Medium**: GitHub API â†’ Token rotation + GraphQL
3. ğŸŸ¡ **Low**: Lambda cold start â†’ Provisioned concurrency

---

**Status**: âœ… Scalability Deep Dive Complete

**Next Steps**:
1. Implement basic auto-scaling (Phase 1)
2. Monitor metrics and identify bottlenecks
3. Scale gradually based on actual load

**Maintained by**: Miyabi Platform Team
**Location**: `/Users/shunsuke/Dev/miyabi-private/.ai/plans/MIYABI_AWS_SCALABILITY_DEEP_DIVE.md`
**Version**: 1.0.0
**Last Updated**: 2025-11-12
