# GPT-OSS-20B Server Setup Guide

OpenAI GPT-OSS-20B ãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒ¼ãƒãƒ¼æ§‹ç¯‰ã‚¬ã‚¤ãƒ‰

---

## ğŸ¯ æ¨å¥¨é †åº

1. **Groq** (5åˆ†) - API ã‚­ãƒ¼ã§å³ã‚¹ã‚¿ãƒ¼ãƒˆ âœ… **æœ€é€Ÿ**
2. **Ollama** (15åˆ†) - ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã€Mac/Linuxå¯¾å¿œ âœ… **ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼**
3. **vLLM** (60åˆ†) - æœ¬ç•ªç’°å¢ƒã€GPU ã‚µãƒ¼ãƒãƒ¼ âœ… **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**

---

## 1ï¸âƒ£ Groq Setup (5åˆ†) âš¡

**ãƒ¡ãƒªãƒƒãƒˆ**: ã‚µãƒ¼ãƒãƒ¼æ§‹ç¯‰ä¸è¦ã€ã‚¯ãƒ©ã‚¦ãƒ‰APIã€1000+ tokens/sec

### Step 1: API ã‚­ãƒ¼å–å¾—

1. Groq Console ã«ã‚¢ã‚¯ã‚»ã‚¹: https://console.groq.com/
2. Sign up / Log in
3. API Keys â†’ Create API Key
4. ã‚­ãƒ¼ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆä¾‹: `gsk_xxxxxxxxxxxxx`ï¼‰

### Step 2: ç’°å¢ƒå¤‰æ•°è¨­å®š

```bash
# ~/.zshrc ã¾ãŸã¯ ~/.bashrc ã«è¿½åŠ 
export GROQ_API_KEY="gsk_xxxxxxxxxxxxx"

# åæ˜ 
source ~/.zshrc
```

### Step 3: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

```bash
# curl ã§ãƒ†ã‚¹ãƒˆ
curl https://api.groq.com/openai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [
      {
        "role": "user",
        "content": "Write a Rust function to calculate factorial"
      }
    ],
    "temperature": 0.2,
    "max_tokens": 512
  }'
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**:
```json
{
  "id": "chatcmpl-xxxxx",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "openai/gpt-oss-20b",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "fn factorial(n: u64) -> u64 { ... }"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 50,
    "total_tokens": 60
  }
}
```

### Step 4: Miyabi ã‹ã‚‰ä½¿ç”¨

```rust
use miyabi_llm::{LLMProvider, GPTOSSProvider, LLMRequest};

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let api_key = std::env::var("GROQ_API_KEY")?;
    let provider = GPTOSSProvider::new_groq(&api_key)?;

    let request = LLMRequest::new("Write a hello world in Rust");
    let response = provider.generate(&request).await?;

    println!("{}", response.text);
    Ok(())
}
```

### æ–™é‡‘

- **Input**: $0.10 / 1M tokens
- **Output**: $0.50 / 1M tokens
- **æœˆé¡ç›®å®‰**: $0.35 (500å›å®Ÿè¡Œ), $3.50 (5,000å›)

---

## 2ï¸âƒ£ Ollama Setup (15åˆ†) ğŸ 

**ãƒ¡ãƒªãƒƒãƒˆ**: ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¯¾å¿œ

### ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶

| é …ç›® | æœ€å°è¦ä»¶ | æ¨å¥¨ |
|------|----------|------|
| RAM | 16GB | 32GB |
| GPU | NVIDIA RTX 3090 (24GB) / Apple M1 16GB | RTX 4090 / M3 Max |
| Storage | 30GB | 50GB |
| OS | macOS 12+, Ubuntu 20.04+ | macOS 14+, Ubuntu 22.04+ |

### Step 1: Ollama ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

**macOS**:
```bash
# Homebrew ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
brew install ollama

# ã¾ãŸã¯å…¬å¼ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼
curl -fsSL https://ollama.com/install.sh | sh
```

**Ubuntu/Linux**:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Step 2: ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```bash
# gpt-oss:20b ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (ç´„16GB)
ollama pull gpt-oss:20b
```

**ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—**:
```
pulling manifest
pulling 4a03f05b1f4a... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 9.5 GB
pulling fe94d09f12cf... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 6.2 GB
pulling 8ab4849b038c... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  254 B
pulling 566c7c09a699... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   94 B
pulling 4ed56a0719af... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  483 B
verifying sha256 digest
writing manifest
success
```

### Step 3: ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ

```bash
# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
ollama run gpt-oss:20b

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›
>>> Write a Rust function to calculate factorial
fn factorial(n: u64) -> u64 {
    match n {
        0 | 1 => 1,
        _ => n * factorial(n - 1),
    }
}
>>> /bye
```

### Step 4: API ã‚µãƒ¼ãƒãƒ¼ãƒ¢ãƒ¼ãƒ‰

```bash
# ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ API ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
ollama serve

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ¼ãƒˆ: http://localhost:11434
```

### Step 5: API ãƒ†ã‚¹ãƒˆ

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "gpt-oss:20b",
  "prompt": "Write a hello world in Rust",
  "stream": false
}'
```

### Step 6: Miyabi ã‹ã‚‰ä½¿ç”¨

```rust
use miyabi_llm::{LLMProvider, GPTOSSProvider, LLMRequest};

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // Ollama provider (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: http://localhost:11434)
    let provider = GPTOSSProvider::new_ollama()?;

    let request = LLMRequest::new("Explain Rust ownership");
    let response = provider.generate(&request).await?;

    println!("{}", response.text);
    Ok(())
}
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ (Apple M3 Max å‚è€ƒ)

- **æ¨è«–é€Ÿåº¦**: 50-100 tokens/sec
- **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**: 5-15ç§’ (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã«ã‚ˆã‚‹)
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨**: 14-16GB

### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

**å•é¡Œ: "Error: model not found"**
```bash
# ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ç¢ºèª
ollama list

# å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ollama pull gpt-oss:20b
```

**å•é¡Œ: "Out of memory"**
```bash
# è»½é‡ãƒ¢ãƒ‡ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆ (é‡å­åŒ–ç‰ˆ)
ollama pull gpt-oss:20b-q4_0  # 4-bit quantized
```

---

## 3ï¸âƒ£ vLLM Setup (60åˆ†) ğŸ­

**ãƒ¡ãƒªãƒƒãƒˆ**: æœ€é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€OpenAI äº’æ› APIã€æœ¬ç•ªç’°å¢ƒå‘ã‘

### ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶

| é …ç›® | æœ€å°è¦ä»¶ | æ¨å¥¨ |
|------|----------|------|
| GPU | NVIDIA A100 40GB | NVIDIA H100 80GB |
| CUDA | 11.8+ | 12.1+ |
| RAM | 64GB | 128GB |
| Storage | 50GB | 100GB |
| OS | Ubuntu 20.04+ | Ubuntu 22.04+ |

### Step 1: NVIDIA Driver + CUDA

```bash
# NVIDIA Driver ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo apt update
sudo apt install nvidia-driver-535

# CUDA Toolkit ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt update
sudo apt -y install cuda

# å†èµ·å‹•
sudo reboot
```

### Step 2: Python ç’°å¢ƒ

```bash
# Python 3.12 + venv
sudo apt install python3.12 python3.12-venv python3-pip

# uv ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (æ¨å¥¨)
curl -LsSf https://astral.sh/uv/install.sh | sh

# ç’°å¢ƒä½œæˆ
uv venv --python 3.12 --seed
source .venv/bin/activate
```

### Step 3: vLLM ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# vLLM with gpt-oss support
uv pip install --pre vllm==0.10.1+gptoss \
  --extra-index-url https://wheels.vllm.ai/gpt-oss/ \
  --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \
  --index-strategy unsafe-best-match
```

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚é–“**: ç´„10-15åˆ†

### Step 4: ãƒ¢ãƒ‡ãƒ«èµ·å‹•

```bash
# API ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
vllm serve openai/gpt-oss-20b

# ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ¼ãƒˆ
vllm serve openai/gpt-oss-20b --port 8080

# GPU æŒ‡å®š
CUDA_VISIBLE_DEVICES=0 vllm serve openai/gpt-oss-20b
```

**èµ·å‹•ãƒ­ã‚°**:
```
INFO:     Started server process [12345]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Loading model openai/gpt-oss-20b
INFO:     Model loaded successfully
```

### Step 5: Docker ãƒ‡ãƒ—ãƒ­ã‚¤ (æ¨å¥¨)

```bash
# Docker ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ãƒ‡ãƒ—ãƒ­ã‚¤
docker run --gpus all \
  -p 8000:8000 \
  --ipc=host \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  vllm/vllm-openai:v0.10.2 \
  --model openai/gpt-oss-20b
```

**Docker Compose**:
```yaml
version: '3.8'

services:
  vllm:
    image: vllm/vllm-openai:v0.10.2
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: --model openai/gpt-oss-20b
    environment:
      - CUDA_VISIBLE_DEVICES=0
    restart: unless-stopped
```

```bash
# èµ·å‹•
docker-compose up -d

# ãƒ­ã‚°ç¢ºèª
docker-compose logs -f
```

### Step 6: API ãƒ†ã‚¹ãƒˆ

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [
      {
        "role": "user",
        "content": "Write a Rust function to calculate factorial"
      }
    ],
    "temperature": 0.2,
    "max_tokens": 512
  }'
```

### Step 7: Miyabi ã‹ã‚‰ä½¿ç”¨

```rust
use miyabi_llm::{LLMProvider, GPTOSSProvider, LLMRequest};

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // vLLM provider
    let provider = GPTOSSProvider::new_vllm("http://localhost:8000")?;

    let request = LLMRequest::new("Explain async/await in Rust");
    let response = provider.generate(&request).await?;

    println!("{}", response.text);
    Ok(())
}
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ (NVIDIA A100 å‚è€ƒ)

- **æ¨è«–é€Ÿåº¦**: 500-1000 tokens/sec
- **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**: 2-3ç§’
- **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: 100+ requests/min

### æœ¬ç•ªç’°å¢ƒè¨­å®š

**Systemd Service** (`/etc/systemd/system/vllm.service`):
```ini
[Unit]
Description=vLLM API Server
After=network.target

[Service]
Type=simple
User=vllm
WorkingDirectory=/opt/vllm
Environment="PATH=/opt/vllm/.venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
Environment="CUDA_VISIBLE_DEVICES=0"
ExecStart=/opt/vllm/.venv/bin/vllm serve openai/gpt-oss-20b --port 8000
Restart=on-failure
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

```bash
# ã‚µãƒ¼ãƒ“ã‚¹æœ‰åŠ¹åŒ–
sudo systemctl daemon-reload
sudo systemctl enable vllm
sudo systemctl start vllm

# ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
sudo systemctl status vllm

# ãƒ­ã‚°ç¢ºèª
sudo journalctl -u vllm -f
```

### Nginx ãƒªãƒãƒ¼ã‚¹ãƒ—ãƒ­ã‚­ã‚·

```nginx
upstream vllm_backend {
    server 127.0.0.1:8000;
}

server {
    listen 80;
    server_name llm.example.com;

    location / {
        proxy_pass http://vllm_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # WebSocket support (for streaming)
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";

        # Timeouts
        proxy_connect_timeout 300s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;
    }
}
```

---

## ğŸ“Š æ¯”è¼ƒè¡¨

| é …ç›® | Groq | Ollama | vLLM |
|------|------|--------|------|
| **ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ™‚é–“** | 5åˆ† | 15åˆ† | 60åˆ† |
| **åˆæœŸã‚³ã‚¹ãƒˆ** | $0 | $0-1,200 (GPU) | $2,000+ (GPU) |
| **æœˆé¡ã‚³ã‚¹ãƒˆ** | $0.35-3.50 | $6.76 (é›»æ°—ä»£) | $539-2,203 (AWS) |
| **æ¨è«–é€Ÿåº¦** | 1000+ t/s | 50-100 t/s | 500-1000 t/s |
| **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·** | 1-2ç§’ | 5-15ç§’ | 2-3ç§’ |
| **ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼** | âŒ ã‚¯ãƒ©ã‚¦ãƒ‰ | âœ… ãƒ­ãƒ¼ã‚«ãƒ« | âœ… ãƒ­ãƒ¼ã‚«ãƒ« |
| **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£** | âœ… è‡ªå‹• | âŒ 1ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | âœ… è¤‡æ•°GPU |
| **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¾å­˜** | âœ… å¿…é ˆ | âŒ ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¯ | âŒ ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¯ |
| **æ¨å¥¨ç”¨é€”** | ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚° | é–‹ç™ºç’°å¢ƒ | æœ¬ç•ªç’°å¢ƒ |

---

## ğŸ¯ æ¨å¥¨ãƒ•ãƒ­ãƒ¼

### ãƒ•ã‚§ãƒ¼ã‚º1: ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚° (Week 1-2)

âœ… **Groq** ã§æ¤œè¨¼
- API ã‚­ãƒ¼å–å¾— (5åˆ†)
- Miyabi çµ±åˆãƒ†ã‚¹ãƒˆ
- ã‚³ã‚¹ãƒˆè©¦ç®—

### ãƒ•ã‚§ãƒ¼ã‚º2: é–‹ç™ºç’°å¢ƒ (Week 3-4)

âœ… **Ollama** ã§é–‹ç™º
- Mac/Linux ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆ
- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ç¢ºä¿

### ãƒ•ã‚§ãƒ¼ã‚º3: æœ¬ç•ªç’°å¢ƒ (Week 5+)

âœ… **vLLM** ã§ãƒ‡ãƒ—ãƒ­ã‚¤
- AWS/GCP GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
- Docker ã‚³ãƒ³ãƒ†ãƒŠåŒ–
- CI/CD çµ±åˆ

---

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### Groq

**å•é¡Œ**: "API key invalid"
```bash
# API ã‚­ãƒ¼ç¢ºèª
echo $GROQ_API_KEY

# å†å–å¾—
# https://console.groq.com/ ã§æ–°ã—ã„ã‚­ãƒ¼ã‚’ç”Ÿæˆ
```

**å•é¡Œ**: "Rate limit exceeded"
```bash
# å¾…æ©Ÿæ™‚é–“ã‚’å¢—ã‚„ã™
sleep 1
```

### Ollama

**å•é¡Œ**: "Connection refused"
```bash
# ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•ç¢ºèª
ps aux | grep ollama

# å†èµ·å‹•
ollama serve
```

**å•é¡Œ**: "Model not found"
```bash
# åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ç¢ºèª
ollama list

# å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ollama pull gpt-oss:20b
```

### vLLM

**å•é¡Œ**: "CUDA out of memory"
```bash
# GPU ãƒ¡ãƒ¢ãƒªç¢ºèª
nvidia-smi

# ãƒãƒƒãƒã‚µã‚¤ã‚ºå‰Šæ¸›
vllm serve openai/gpt-oss-20b --max-num-batched-tokens 2048
```

**å•é¡Œ**: "Model not found on HuggingFace"
```bash
# HuggingFace ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
export HF_TOKEN="hf_xxxxx"

# å†èµ·å‹•
vllm serve openai/gpt-oss-20b
```

---

## ğŸ“š å‚è€ƒãƒªãƒ³ã‚¯

- [Groq Console](https://console.groq.com/)
- [Ollama Official](https://ollama.com/)
- [vLLM Documentation](https://docs.vllm.ai/)
- [GPT-OSS Model Card](https://huggingface.co/openai/gpt-oss-20b)
- [OpenAI Cookbook - vLLM](https://cookbook.openai.com/articles/gpt-oss/run-vllm)
- [OpenAI Cookbook - Ollama](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)

---

**æœ€çµ‚æ›´æ–°**: 2025-10-17
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: âœ… å®Ÿè£…ã‚¬ã‚¤ãƒ‰å®Œæˆ
